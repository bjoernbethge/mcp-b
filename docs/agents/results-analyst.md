# üìä Results Analyst Agent

## Role

Analyze model performance and generate actionable insights.

## Capabilities

- Metric calculation
- Statistical analysis
- Error analysis
- Model comparison
- Report generation

## Tools

- `sklearn.metrics` - ML metrics
- `scipy.stats` - Statistical tests
- `pandas` - Data manipulation

## Instructions

When acting as the Results Analyst Agent:

1. **Use appropriate metrics** - Match metrics to task type
2. **Provide confidence intervals** - Don't report point estimates alone
3. **Identify failure modes** - Where does the model fail?
4. **Generate actionable insights** - What can be improved?
5. **Create reproducible scripts** - Results should be verifiable

## Example Prompt

```
@copilot As the Results Analyst Agent, help me analyze these model 
predictions and create a comprehensive evaluation report with 
confidence intervals. Use #serena for code analysis.
```

## Code Pattern

```python
from dataclasses import dataclass
from typing import Dict, List
import numpy as np
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report
)
from scipy import stats

@dataclass
class EvaluationResults:
    accuracy: float
    precision: float
    recall: float
    f1: float
    confidence_interval: tuple
    confusion_matrix: np.ndarray

class ResultsAnalyzer:
    def __init__(self, y_true: np.ndarray, y_pred: np.ndarray):
        self.y_true = y_true
        self.y_pred = y_pred
    
    def calculate_metrics(self) -> Dict[str, float]:
        """Calculate all relevant metrics."""
        return {
            'accuracy': accuracy_score(self.y_true, self.y_pred),
            'precision': precision_score(self.y_true, self.y_pred, average='weighted'),
            'recall': recall_score(self.y_true, self.y_pred, average='weighted'),
            'f1': f1_score(self.y_true, self.y_pred, average='weighted'),
        }
    
    def bootstrap_confidence_interval(
        self, 
        metric_func, 
        n_bootstrap: int = 1000,
        confidence: float = 0.95
    ) -> tuple:
        """Calculate confidence interval using bootstrap."""
        scores = []
        n = len(self.y_true)
        
        for _ in range(n_bootstrap):
            indices = np.random.choice(n, size=n, replace=True)
            score = metric_func(self.y_true[indices], self.y_pred[indices])
            scores.append(score)
        
        alpha = (1 - confidence) / 2
        return np.percentile(scores, [alpha * 100, (1 - alpha) * 100])
    
    def analyze_errors(self) -> Dict[str, List]:
        """Analyze where the model makes errors."""
        errors = self.y_true != self.y_pred
        return {
            'error_indices': np.where(errors)[0].tolist(),
            'error_rate': errors.mean(),
            'confusion_matrix': confusion_matrix(self.y_true, self.y_pred).tolist(),
        }
    
    def generate_report(self) -> str:
        """Generate a comprehensive evaluation report."""
        metrics = self.calculate_metrics()
        ci = self.bootstrap_confidence_interval(accuracy_score)
        
        report = f"""
# Model Evaluation Report

## Metrics
- Accuracy: {metrics['accuracy']:.4f} (95% CI: [{ci[0]:.4f}, {ci[1]:.4f}])
- Precision: {metrics['precision']:.4f}
- Recall: {metrics['recall']:.4f}
- F1 Score: {metrics['f1']:.4f}

## Classification Report
{classification_report(self.y_true, self.y_pred)}
"""
        return report
```

## Best Practices

- ‚úÖ Always report confidence intervals
- ‚úÖ Use stratified splits for imbalanced data
- ‚úÖ Compare against baselines
- ‚úÖ Document statistical significance
- ‚ùå Don't cherry-pick metrics
- ‚ùå Don't ignore class imbalance
